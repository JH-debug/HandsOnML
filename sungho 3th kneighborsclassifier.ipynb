{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "                     metric_params=None, n_jobs=None, n_neighbors=3, p=2,\n",
       "                     weights='uniform')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = [[0], [1], [2], [3]]\n",
    "y = [0, 0, 1, 1]\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "neigh = KNeighborsClassifier(n_neighbors=3)\n",
    "neigh.fit(X, y) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. n_neighbors : int, optional (default = 5)\n",
    "               kneighbors 쿼리에 기본적으로 사용할 이웃의 수입니다 .\n",
    "weights : str or callable, optional (default = ‘uniform’)\n",
    "예측에 사용되는 가중치 함수. 가능한 값 :\n",
    "1)‘uniform’: 균일 한 가중치. 각 이웃의 모든 포인트는 동일하게 가중치가 부여됩니다.\n",
    "\n",
    "2)‘distance’: 거리의 역수로 가중치를 나타냅니다. 이 경우, 쿼리 지점의 인접 이웃은 멀리 떨어진 이웃보다 더 큰 영향을 미칩니다.\n",
    "\n",
    "3) [callable] : 거리 배열을 허용하고 가중치를 포함하는 동일한 모양의 배열을 반환하는 사용자 정의 함수입니다.\n",
    "\n",
    "2. algorithm : {‘auto’, ‘ball_tree’, ‘kd_tree’, ‘brute’}, optional\n",
    "가장 가까운 이웃을 계산하는 데 사용되는 알고리즘 :\n",
    "1)'ball_tree' =  BallTree\n",
    "2)'kd_tree' =  KDTree\n",
    "3)'brute' =  a brute-force search.\n",
    "4)'auto'는 fit메소드에 전달 된 값을 기반으로 가장 적합한 알고리즘을 결정하려고 시도합니다 .\n",
    "참고 : 스파 스 입력에 피팅하면 무차별 대입을 사용하여이 매개 변수 설정이 무시됩니다.\n",
    "\n",
    "3. leaf_size : int, optional (default = 30)\n",
    "잎 크기는 BallTree 또는 KDTree로 전달되었습니다. 이는 트리를 저장하는 데 필요한 메모리뿐만 아니라 구성 및 쿼리 속도에 영향을 줄 수 있습니다. 최적의 값은 문제의 특성에 따라 다릅니다.\n",
    "\n",
    "4. p : integer, optional (default = 2)\n",
    "Minkowski 메트릭의 전원 매개 변수. p = 1일 때, 이는 p = 2에 manhattan_거리(l1) 및 유클리드_거리(l2)를 사용하는 것과 같다. 임의 p의 경우 minkowski_거리(l_p)를 사용한다.\n",
    "\n",
    "5. metric : string or callable, default ‘minkowski’\n",
    "트리에 사용할 거리 측정 항목 기본 메트릭은 minkowski이며 p = 2 인 표준 유클리드 메트릭과 같습니다. 사용 가능한 메트릭 목록은 DistanceMetric 클래스 설명서를 참조하십시오.\n",
    "\n",
    "6. metric_params : dict, optional (default = None)\n",
    "메트릭 함수에 대한 추가 키워드 인수\n",
    "\n",
    "7. n_jobs : int or None, optional (default=None)\n",
    "이웃 검색을 위해 실행할 병렬 작업 수입니다. 문맥이 None아닌 한 1을 의미합니다 joblib.parallel_backend. -1모든 프로세서를 사용하는 것을 의미합니다. 자세한 내용은 용어집 을 참조하십시오. fit방법에 영향을 미치지 않습니다 ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n"
     ]
    }
   ],
   "source": [
    "print(neigh.predict([[1.1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.66666667 0.33333333]]\n"
     ]
    }
   ],
   "source": [
    "print(neigh.predict_proba([[0.9]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. fit(self, X, y) : X를 훈련 데이터로 사용하고 y를 목표 값으로 사용하여 모형을 적합\n",
    "2. get_params(self[, deep]) : 평가자에 대한 매개변수를 구하십시오.\n",
    "3. kneighbors(self[, X, n_neighbors, …])\tFinds the K-neighbors of a point.\n",
    "4. kneighbors_graph(self[, X, n_neighbors, mode]) : X의 점에 대한 k- 이웃의 (가중) 그래프를 계산합니다\n",
    "5. predict(self, X) : 제공된 데이터의 클래스 레이블을 예측\n",
    "6. predict_proba(self, X) : 테스트 데이터 X에 대한 확률 추정값을 반환합니다.\n",
    "7. score(self, X, y[, sample_weight]) : 주어진 테스트 데이터 및 레이블의 평균 정확도를 반환합니다.\n",
    "8. set_params(self, \\*\\*params) :이 추정기의 매개 변수를 설정하십시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[0.5]]), array([[2]], dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "samples = [[0., 0., 0.], [0., .5, 0.], [1., 1., .5]]\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "neigh = NearestNeighbors(n_neighbors=1)\n",
    "neigh.fit(samples) \n",
    "print(neigh.kneighbors([[1., 1., 1.]])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-10-92da96cba4dd>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-10-92da96cba4dd>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    X  =  [[ 0 ,,  1. ,  0. ],  [ 1. ,  0. ,  1. ]]\u001b[0m\n\u001b[1;37m                ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "X  =  [[ 0 ,,  1. ,  0. ],  [ 1. ,  0. ,  1. ]] \n",
    "neigh.kneighbors(X, return_distance = False )  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음 예제에서는 데이터 세트를 나타내는 배열에서 NeighborsClassifier 클래스를 구성하고 [1,1,1]에 가장 가까운 사람을 묻습니다.\n",
    "보다시피, [[0.5]]와 [[2]]를 반환합니다. 이는 요소가 0.5 거리에 있고 샘플의 세 번째 요소임을 의미합니다 (인덱스는 0에서 시작). 여러 포인트를 쿼리 할 수도 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [1., 0., 1.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = [[0], [3], [1]]\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "neigh = NearestNeighbors(n_neighbors=2)\n",
    "neigh.fit(X) \n",
    "A = neigh.kneighbors_graph(X)\n",
    "A.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. predict( self , X ) : 공된 데이터의 클래스 레이블을 예측\n",
    "2. predict_proba( self , X ) : 테스트 데이터 X에 대한 확률 추정값을 반환합니다.\n",
    "3. score(self, X, y, sample_weight=None) : 주어진 테스트 데이터 및 레이블의 평균 정확도를 반환합니다.다중 레이블 분류에서 이는 각 레이블 집합을 올바르게 예측해야하는 각 샘플에 대해 필요하기 때문에 가혹한 메트릭 인 하위 집합 정확도입니다.\n",
    "4. set_params( self , ** params ) : 이 방법은 파이프 라인과 같은 중첩 된 개체뿐만 아니라 간단한 견적 도구에서도 작동합니다. 후자는 폼의 매개 변수를 가지 <component>__<parameter>므로 중첩 객체의 각 구성 요소를 업데이트 할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatically created module for IPython interactive environment\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(__doc__)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn import neighbors, datasets\n",
    "\n",
    "n_neighbors = 15\n",
    "\n",
    "# import some data to play with\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "# we only take the first two features. We could avoid this ugly\n",
    "# slicing by using a two-dim dataset\n",
    "X = iris.data[:, :2]\n",
    "y = iris.target\n",
    "\n",
    "h = .02  # step size in the mesh\n",
    "\n",
    "# Create color maps\n",
    "cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])\n",
    "cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])\n",
    "\n",
    "for weights in ['uniform', 'distance']:\n",
    "    # we create an instance of Neighbours Classifier and fit the data.\n",
    "    clf = neighbors.KNeighborsClassifier(n_neighbors, weights=weights)\n",
    "    clf.fit(X, y)\n",
    "\n",
    "    # Plot the decision boundary. For that, we will assign a color to each\n",
    "    # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "    # Put the result into a color plot\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.figure()\n",
    "    plt.pcolormesh(xx, yy, Z, cmap=cmap_light)\n",
    "\n",
    "    # Plot also the training points\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold,\n",
    "                edgecolor='k', s=20)\n",
    "    plt.xlim(xx.min(), xx.max())\n",
    "    plt.ylim(yy.min(), yy.max())\n",
    "    plt.title(\"3-Class classification (k = %i, weights = '%s')\"\n",
    "              % (n_neighbors, weights))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
